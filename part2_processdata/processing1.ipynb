{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dd9426-3a7a-4d15-a995-93efafa83f4f",
   "metadata": {},
   "source": [
    "## Program: preprocess tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import fasttext\n",
    "# Fasttext references:\n",
    "#[1] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\n",
    "#[2] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, T. Mikolov, FastText.zip: Compressing text classification models\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef31c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_valid_tweets(df):\n",
    "    valid_tweets = df[\n",
    "        df[\"Tweet\"].notna()  # No NaN or None\n",
    "        & df[\"Tweet\"].apply(lambda x: isinstance(x, str))  # It's a string\n",
    "        & df[\"Tweet\"].apply(lambda x: not (isinstance(x, str) and (x.isspace() or len(x) == 0)))  # No blank spaces or empty strings\n",
    "    ]\n",
    "    valid_tweets.reset_index(drop=True, inplace=True)\n",
    "    return valid_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_filtered_tweets(df, politics = None, date_ini = None, date_end = None, lang = None):\n",
    "\n",
    "    if politics is not None and len(politics) > 0:\n",
    "        filter_politics = df.Tweet.str.contains(politics[0], case=False)\n",
    "        for keyword in politics[1:]:\n",
    "            filter_politics |= df.Tweet.str.contains(keyword, case=False)\n",
    "        df = df[filter_politics].copy()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if date_ini is not None:\n",
    "        filter_date_ini = df.Date >= date_ini\n",
    "        df = df[filter_date_ini].copy()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if date_end is not None:\n",
    "        filter_date_end = df.Date <= date_end\n",
    "        df = df[filter_date_end].copy()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lang is not None:\n",
    "        model_path = r\"lid.176.bin\"  # Downloaded model that is better for tweets (Facebook's model)\n",
    "        model = fasttext.load_model(model_path)\n",
    "        clean_list = [re.sub(r\"RT @\\w+\", \"\", s.replace(\"\\n\", \"\")) for s in list(df[\"Tweet\"])] # the model does not accept new lines\n",
    "        language_lists = model.predict(clean_list, k=1)[0]\n",
    "        filter_english = [sublist == ['__label__en'] for sublist in language_lists] # English tweets\n",
    "        print(\"Not English tweets: \", len(filter_english) - sum(filter_english))\n",
    "        # print(df[~pd.Series(filter_english)])\n",
    "        df = df[filter_english].copy()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../fase1_descargadatos/combined_table_users_tweets.csv\"\n",
    "df_24 = pd.read_csv(dir)\n",
    "\n",
    "# convert Date column to datetime\n",
    "df_24['Date'] = pd.to_datetime(df_24['Date'])\n",
    "print(df_24.shape)\n",
    "df_24.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0730b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24 = keep_valid_tweets(df_24)\n",
    "df_24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe688297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24 = keep_filtered_tweets(df_24, politics = [\"@realDonaldTrump\", \"@KamalaHarris\", \"@JoeBiden\"], date_ini = pd.to_datetime(\"2024-10-29\"), date_end = pd.to_datetime(\"2024-11-05\"), lang = True) # we have dates from the 28th but we only want 8 days like the rest of the years\n",
    "df_24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_16 = \"../fase1_descargadatos/usa1620/tweets16_common.csv\"\n",
    "dir_20 = \"../fase1_descargadatos/usa1620/tweets20_common.csv\"\n",
    "\n",
    "df_16_raw = pd.read_csv(dir_16)\n",
    "df_20_raw = pd.read_csv(dir_20)\n",
    "\n",
    "print(df_16_raw.shape)\n",
    "print(df_20_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e257f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep useful columns and rename them\n",
    "df_16 = df_16_raw[[\"screen_name\", \"text\", \"created_at\"]].copy()\n",
    "df_16.rename(columns={\"screen_name\": \"User\", \"text\": \"Tweet\", \"created_at\": \"Date\"}, inplace=True)\n",
    "\n",
    "df_20 = df_20_raw[[\"screen_name\", \"text\", \"created_at\"]].copy()\n",
    "df_20.rename(columns={\"screen_name\": \"User\", \"text\": \"Tweet\", \"created_at\": \"Date\"}, inplace=True)\n",
    "\n",
    "# convert Date column to datetime\n",
    "df_16['Date'] = pd.to_datetime(df_16['Date'].str[:10])\n",
    "df_20['Date'] = pd.to_datetime(df_20['Date'].str[:10])\n",
    "\n",
    "# keep valid tweets\n",
    "df_16 = keep_valid_tweets(df_16)\n",
    "df_20 = keep_valid_tweets(df_20)\n",
    "\n",
    "# keep filtered tweets (it wasn't done for all ej 2026 tw: 2815508 didnt have any politics keyword)\n",
    "df_16 = keep_filtered_tweets(df_16, politics = [\"@realDonaldTrump\", \"@HillaryClinton\"], date_end = pd.to_datetime(\"2016-11-08\"), lang = True) # tenemos desde el 1\n",
    "df_20 = keep_filtered_tweets(df_20, politics = [\"@realDonaldTrump\", \"@JoeBiden\"], date_end = pd.to_datetime(\"2020-11-03\"), lang = True) # tenemos desde el 27\n",
    "\n",
    "# print\n",
    "print(df_16.shape)\n",
    "print(df_16.head())\n",
    "\n",
    "print(df_20.shape)\n",
    "print(df_20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection of users\n",
    "\n",
    "# Get unique users from each dataframe\n",
    "users_16 = set(df_16[\"User\"].unique())\n",
    "users_20 = set(df_20[\"User\"].unique())\n",
    "users_24 = set(df_24[\"User\"].unique())\n",
    "\n",
    "# Calculate the intersection of users\n",
    "user_intersection = users_24.intersection(users_16).intersection(users_20)\n",
    "\n",
    "# we filter the dataframes\n",
    "df_16_filtered = df_16[df_16[\"User\"].isin(user_intersection)]\n",
    "df_20_filtered = df_20[df_20[\"User\"].isin(user_intersection)]\n",
    "df_24_filtered = df_24[df_24[\"User\"].isin(user_intersection)]\n",
    "\n",
    "df_16_filtered.reset_index(drop=True, inplace=True)\n",
    "df_20_filtered.reset_index(drop=True, inplace=True)\n",
    "df_24_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_16_filtered.shape)\n",
    "print(df_20_filtered.shape)\n",
    "print(df_24_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5723792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the three dataframes\n",
    "df_16_filtered.to_csv(\"tweets16_filtered.csv\", index=False)\n",
    "df_20_filtered.to_csv(\"tweets20_filtered.csv\", index=False)\n",
    "df_24_filtered.to_csv(\"tweets24_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look for the tweets that are repeated the most to classify those manually\n",
    "repeated_tweets = []\n",
    "repeated_tweets.append(df_16_filtered[\"Tweet\"].value_counts().head(100))\n",
    "repeated_tweets.append(df_20_filtered[\"Tweet\"].value_counts().head(100))\n",
    "repeated_tweets.append(df_24_filtered[\"Tweet\"].value_counts().head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eedb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we keep the most repeated tweets, we would have 300 tweets to classify manually and this would represent a greater amount of the total tweets\n",
    "# let's see many tweets we would be classifying if we classified these 300 tweets\n",
    "\n",
    "df_16_filtered[\"Tweet\"].value_counts().head(100).sum() + df_20_filtered[\"Tweet\"].value_counts().head(100).sum() + df_24_filtered[\"Tweet\"].value_counts().head(100).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24_filtered[\"Tweet\"].value_counts().head(300).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"repeated_tweets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in repeated_tweets:\n",
    "        writer.writerow([item])  # Writing each item in a new row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
